{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98998a55-1347-49b4-bc8e-fb9424274edd",
   "metadata": {},
   "source": [
    "# Project (final assessment of STK 353)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import Datasets\n",
    "from Datasets import Source_Code\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string # for translating strings to not have any punctuation\n",
    "import nltk # for lemmatization\n",
    "\n",
    "# get data from source code\n",
    "Honda_accord_2008_data = Source_Code.send_honda_accord_2008_data()\n",
    "Honda_accord_2009_data = Source_Code.send_honda_accord_2009_data()\n",
    "Hyundai_sonata_2009_data = Source_Code.send_hyundai_sonata_2008_data()\n",
    "Toyota_corolla_2009_data = Source_Code.send_toyota_corolla_2009_data() \n",
    "\n",
    "type(Honda_accord_2008_data)\n",
    "#Toyota_corolla_2009_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stan branch just pushed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924b2b84-8ff6-4989-9c27-a0c7c10a37d0",
   "metadata": {},
   "source": [
    "Let's discuss the car datasets `2009_honda_accord`, `2009_hyundai_sonata`, and `2009_toyota_corolla`. These datasets contain customer reviews for three different car models from 2009. \n",
    "\n",
    "1. As a stakeholder representing these companies, your objective is to determine which car has acheived the highest customer satisfaction rate among the three.\n",
    "\n",
    "2. Additionally, you want to investigate whether the satisfaction rate for the Honda Accord has improved from 2008 to 2009. To answer this question, you will also need to analyze the dataset `2008_honda_accord`, which consists of customer reviews for the Honda Accord from the year 2008.\n",
    "\n",
    "3. In the end, your final goal is to categorize the customer reviews for the Honda Accord (only those of the year 2008) into $k$ meaningful groups.\n",
    "\n",
    "To answer the above questions, please consider the following points:\n",
    "- a) Obtain the number of comments given in all four reviews and report the results.\n",
    "- b) In the cleaning process, using lemmatization rather than stemming is recommended (Why?)\n",
    "- c) During the cleaning process, you also need to remove numbers and HTML tags e.g. `<DOC>, <TEXT>, <AUTHOR>` and similar elements.\n",
    "- d)  The first two questions can be solved using sentiment analysis techniques.\n",
    "- e) Create appropriate `wordcloud`s for each part to visualize the most frequent words in the reviews.\n",
    "- f) Determine the optimal value of $k$ from the set $\\{2, 3, 4, 5\\}$ in Question 3.\n",
    "- g) (_optional_) Feel free to obtain any other relevant outputs, such as evaluation metrics or additional plots.\n",
    "- h) (_optional_) If you find that the algorithm used in Question 3 does not provide satisfactory performance, you can try incorporating the datasets `2008_honda_accord` and `2009_honda_accord` to cluster the reviews again, aiming for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbb24fe-2a0c-4313-8366-7516f7976d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540 224 262 226\n"
     ]
    }
   ],
   "source": [
    "# a) Obtain the number of comments given in all four reviews and report the results.\n",
    "\n",
    "# Count the number of occurrences using a regex to count the number of comments\n",
    "accord_08_comment_count = len(Honda_accord_2008_data['doc'].isna())\n",
    "accord_09_comment_count =  len(Honda_accord_2009_data['doc'].isna())\n",
    "sonata_09_comment_count =  len(Hyundai_sonata_2009_data['doc'].isna())\n",
    "corolla_09_comment_count =  len(Toyota_corolla_2009_data['doc'].isna())\n",
    "\n",
    "#print results\n",
    "print(accord_08_comment_count, accord_09_comment_count, sonata_09_comment_count, corolla_09_comment_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- b) In the cleaning process, using lemmatization rather than stemming is recommended (Why?)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['turned',\n",
       " '90k',\n",
       " 'still',\n",
       " 'pleased',\n",
       " '2nd',\n",
       " 'set',\n",
       " 'ive',\n",
       " 'owned',\n",
       " 'three',\n",
       " 'vehicles',\n",
       " 'prior',\n",
       " 'acco',\n",
       " 'purchased',\n",
       " 'honda',\n",
       " 'accord',\n",
       " 'due',\n",
       " 'reviews',\n",
       " 'leased',\n",
       " 'accord',\n",
       " 'april',\n",
       " 'overall',\n",
       " 'pretty',\n",
       " 'nice',\n",
       " 'car',\n",
       " 'model',\n",
       " 'c',\n",
       " 'quiet',\n",
       " 'lexus',\n",
       " 'quiet',\n",
       " 'enough',\n",
       " 'gm',\n",
       " 'truck',\n",
       " 'leases',\n",
       " 'years',\n",
       " '3rd',\n",
       " 'accord',\n",
       " 'ive',\n",
       " 'owned',\n",
       " 'p',\n",
       " 'love',\n",
       " 'love',\n",
       " 'love',\n",
       " '190hp',\n",
       " 'uncertain',\n",
       " 'driving',\n",
       " 'accord',\n",
       " 'ye',\n",
       " 'name',\n",
       " 'length',\n",
       " 'dtype',\n",
       " 'object']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#- c) During the cleaning process, you also need to remove numbers and HTML tags e.g. `<DOC>, <TEXT>, <AUTHOR>` and similar elements.\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Step 1: remove the spaces\n",
    "def remove_spaces(dataframe):\n",
    "    df_sans_spaces = dataframe.applymap(lambda x: x.replace('\\n', ''))\n",
    "    return df_sans_spaces\n",
    "\n",
    "\n",
    "\n",
    "# Step 2:  remove the punctuation\n",
    "def rm_punctuation(comment_salad_string):\n",
    "    comments_word_library = comment_salad_string.split() # split the words into individual strings\n",
    "    translator = str.maketrans('', '', string.punctuation) # create a table transaltion table to remove the punctuation\n",
    "    sans_punctuation_words = [word.translate(translator) for word in comments_word_library] # Removes the punctuation\n",
    "    return sans_punctuation_words\n",
    "\n",
    "\n",
    "# Step 3: Remove stopwords\n",
    "def remove_numbers_n_words(word_lib):\n",
    "    # Define a regular expression pattern to match single digits and 'doc' (case-insensitive)\n",
    "    pattern = r'\\d\\b|\\bdoc\\b'\n",
    "    \n",
    "    # Use list comprehension to filter out undesired elements\n",
    "    filtered_words = [word for word in word_lib if not re.search(pattern, word, re.I)]\n",
    "\n",
    "    # Join the filtered words back into a single string or keep as a list\n",
    "    filtered_text = ' '.join(filtered_words)    \n",
    "    \n",
    "    return filtered_text.split()\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: remove the stopwords\n",
    "def rm_stopwords(comment_salad_string_list):\n",
    "    # Remove stopwords from the list of words\n",
    "    filtered_words = [word for word in comment_salad_string_list if word.lower() not in STOPWORDS]\n",
    "    \n",
    "    # Convert each word to lowercase using list comprehension\n",
    "    lowercase_words = [word.lower() for word in filtered_words]  \n",
    "    \n",
    "    return lowercase_words # return list of words\n",
    "\n",
    "\n",
    "# Step 5: get remaining clean words\n",
    "def get_cleaned_words(dataframe):\n",
    "    # send dataframe to remove the empty spaces in the series rows\n",
    "    sans_spaces_df = remove_spaces(dataframe)\n",
    "    \n",
    "    # get comments column from dataframe and make it a big string\n",
    "    comment_str = str(sans_spaces_df['doc'])\n",
    "    \n",
    "    # remove the punctuation from the string\n",
    "    sans_punctuation_comment_string_list = rm_punctuation(comment_str)\n",
    "    \n",
    "    # remove the punctuation from the string\n",
    "    sans_numbers_words = remove_numbers_n_words(sans_punctuation_comment_string_list)\n",
    "    \n",
    "    # remove the stopwords\n",
    "    sans_stopwords_words = rm_stopwords(sans_numbers_words)\n",
    "    \n",
    "    # get cleaned string of words\n",
    "    cleaned_comments_string_list = sans_stopwords_words\n",
    "    \n",
    "    return cleaned_comments_string_list # return string of cleaned words\n",
    "    \n",
    "\n",
    "# get resulting output\n",
    "clean_words = get_cleaned_words(Honda_accord_2008_data)\n",
    "clean_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- d)  The first two questions can be solved using sentiment analysis techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- e) Create appropriate `wordcloud`s for each part to visualize the most frequent words in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- f) Determine the optimal value of k from the set {2, 3, 4, 5\\} in Question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- g) (_optional_) Feel free to obtain any other relevant outputs, such as evaluation metrics or additional plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#- h) (_optional_) If you find that the algorithm used in Question 3 does not provide satisfactory performance, you can try incorporating the datasets `2008_honda_accord` and `2009_honda_accord` to cluster the reviews again, aiming for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
